{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_MDP_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TJxG40-LVV8"
      },
      "source": [
        "#Solving a grid problem with the transition and reward (description of the environment given in csv file) using synchronous dynamic programming.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AcCC0_c0XJU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uacgIVi66DDI"
      },
      "source": [
        "#import data in the form of csv file\n",
        "trans_data = pd.read_csv('transitions_data.csv',header=None)\n",
        "#rd_data = pd.read_csv('rewards.csv',header=None)\n",
        "rwd_data = pd.read_csv('rewards.csv',header=None)\n",
        "#convert the pandas file into numpy array or matrix\n",
        "trans_data = trans_data.to_numpy()\n",
        "#t_data = pd.read_csv('transitions_data.csv',header=None)\n",
        "rwd_data = rwd_data.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsfZhnoEH-i3"
      },
      "source": [
        "#print(rd_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-nyi3jO2JE6"
      },
      "source": [
        "#print(trans_data)\n",
        "#print(rwd_data)\n",
        "gamma = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk6um68B2O-V",
        "outputId": "41a47a90-627e-49b4-a6c4-c93295b59c70"
      },
      "source": [
        "#preparation of the input data and storing it into a np matrix\n",
        "transitions = {}\n",
        "len_data = trans_data.shape[0]\n",
        "td = trans_data\n",
        "for i in range(1,len_data):\n",
        "  if (td[i][0] in transitions):\n",
        "    if td[i][1] in transitions[td[i][0]]:\n",
        "      transitions[td[i][0]][td[i][1]].append((float(td[i][3]),td[i][2]))\n",
        "    else:\n",
        "      transitions[td[i][0]][td[i][1]] = [(float(td[i][3]),td[i][2])]\n",
        "  else:\n",
        "    transitions[td[i][0]] = {td[i][1]:[(float(td[i][3]),td[i][2])]}\n",
        "#print(trans_data.shape)\n",
        "#for i in transitions.keys():\n",
        "  #print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3 0)\n",
            "(3 1)\n",
            "(1 0)\n",
            "(2 1)\n",
            "(1 2)\n",
            "(2 0)\n",
            "(3 2)\n",
            "(2 2)\n",
            "(0 1)\n",
            "(0 0)\n",
            "(0 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svMbVjID6xKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f81e56-b5cf-4fdc-f8fc-2e7e1f770dda"
      },
      "source": [
        "rewards = {}\n",
        "rd = rwd_data\n",
        "len_rewards = rd.shape[0]\n",
        "for i in range(0,len_rewards):\n",
        "  rewards[rd[i][0]] = float(rd[i][1]) if rd[i][1] != 'None' else np.nan\n",
        "#print(len(rewards.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng46TunBFbbW",
        "outputId": "58059cae-ff51-4e3f-8730-b0496af35a0f"
      },
      "source": [
        "rkeys = rewards.keys()\n",
        "tkeys = transitions.keys()\n",
        "st = ''\n",
        "st2 = ''\n",
        "for i in tkeys:\n",
        "  st += i + ' '\n",
        "for j in rkeys:\n",
        "  st2 += j + ' '\n",
        "print(st)\n",
        "print(st2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3 0) (3 1) (1 0) (2 1) (1 2) (2 0) (3 2) (2 2) (0 1) (0 0) (0 2) \n",
            "(0 1) (3 0) (0 0) (1 0) (3 1) (1 1) (2 1) (1 2) (2 0) (3 2) (0 2) (2 2) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXU0_Edjq-Tp"
      },
      "source": [
        "#This MDP class is to define the environment with which our agent is interacting\n",
        "\n",
        "class MarkovDecisionProcess:\n",
        "  def __init__(self, states=[], transition={}, reward={}, gamma=0.9):\n",
        "    self.states = states\n",
        "    self.transition = transition\n",
        "    self.reward = reward\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def Rwd(self, state):\n",
        "    return self.reward[state]\n",
        "  \n",
        "  def Trans(self, state, action):\n",
        "    return self.transition[state][action]\n",
        "\n",
        "  def action(self, state):\n",
        "    return self.transition[state].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq4IUU7X-9fH",
        "outputId": "453a4c7b-41b6-480c-ef7f-508b2f842a7b"
      },
      "source": [
        "Transitions = transitions\n",
        "Rewards = rewards\n",
        "States = transitions.keys()\n",
        "#print(States)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['(3 0)', '(3 1)', '(1 0)', '(2 1)', '(1 2)', '(2 0)', '(3 2)', '(2 2)', '(0 1)', '(0 0)', '(0 2)'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntO6gw9qsb-i"
      },
      "source": [
        "mdp = MarkovDecisionProcess(states = States, transition = Transitions, reward = Rewards)\n",
        "epsilon = 0.2\n",
        "def val_iteration():\n",
        "  states = mdp.states\n",
        "  actions = mdp.action\n",
        "  Trans = mdp.Trans\n",
        "  Rwd = mdp.Rwd\n",
        "\n",
        "  #initializing the policy all with 0 value\n",
        "  V1 = {s: 0 for s in states}\n",
        "  while True:\n",
        "    V = V1.copy()\n",
        "    delta = 0 \n",
        "\n",
        "    for s in states:\n",
        "      #Synchrounous Bellman update, updating the utility values\n",
        "      #two times or nested list comprehension done two times\n",
        "      V1[s] = Rwd(s) + gamma * max([sum([p*V[s1] for (p,s1) in Trans(s,a)]) for a in actions(s)])\n",
        "      #calculating the max difference in subsequent iterations \n",
        "      delta = max(delta, abs(V1[s]-V[s]))\n",
        "\n",
        "    if (delta < epsilon*(1-gamma)/gamma):\n",
        "      return V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkcuuO__vql_"
      },
      "source": [
        "def expected_utility(a,s,V):\n",
        "  Trans = mdp.Trans\n",
        "  return sum([p*V[s1] for (p,s1) in Trans(s,a)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0Wv1oEwvISi"
      },
      "source": [
        "def best_policy(V):\n",
        "  states = mdp.states\n",
        "  actions = mdp.action\n",
        "  pi_policy = {}\n",
        "  for s in states:\n",
        "    pi_policy[s] = max(actions(s), key=lambda a: expected_utility(a,s,V))\n",
        "  return pi_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDBX0QPfv7jD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b7f517-93e7-4413-c7de-f1f5cb80969f"
      },
      "source": [
        "V = val_iteration()\n",
        "print(\"State-Value\")\n",
        "for s in V:\n",
        "  print(s,' - ',V[s])\n",
        "pi = best_policy(V)\n",
        "print(\"\\n Optimal policy is \\n State - Action \")\n",
        "for s in V:\n",
        "  print(s,\" \",'-',\" \",pi[s])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State-Value\n",
            "(3 0)  -  2.6118518779566258\n",
            "(3 1)  -  -10.0\n",
            "(1 0)  -  4.0484603014043845\n",
            "(2 1)  -  5.633133936112791\n",
            "(1 2)  -  7.348949178462026\n",
            "(2 0)  -  4.611938876757595\n",
            "(3 2)  -  10.0\n",
            "(2 2)  -  8.425245372413883\n",
            "(0 1)  -  5.49111436850307\n",
            "(0 0)  -  4.691426415451711\n",
            "(0 2)  -  6.312795272532248\n",
            "\n",
            " Optimal policy is \n",
            " State - Action \n",
            "(3 0)   -   L\n",
            "(3 1)   -   EXIT\n",
            "(1 0)   -   L\n",
            "(2 1)   -   U\n",
            "(1 2)   -   R\n",
            "(2 0)   -   U\n",
            "(3 2)   -   EXIT\n",
            "(2 2)   -   R\n",
            "(0 1)   -   U\n",
            "(0 0)   -   U\n",
            "(0 2)   -   R\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcTIctKHK1Vz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}